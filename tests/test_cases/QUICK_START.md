# Quick Start Guide - Test Case Organization

## What Was Done

Your test .docx files have been organized into a clean, systematic structure:

### Before (Messy):
```
tests/fixtures/
‚îú‚îÄ‚îÄ test_main_document.docx          ‚ùì What is this?
‚îú‚îÄ‚îÄ test_fallback_requirements.docx  ‚ùì What does this relate to?
‚îú‚îÄ‚îÄ test_complex_fallback.docx       ‚ùì Which input does this use?
‚îî‚îÄ‚îÄ sample_fallback_contract.docx    ‚ùì Unused?

archive/debug_word_processor/
‚îî‚îÄ‚îÄ sample_input.docx                ‚ùì Has tracked changes?
```

### After (Organized):
```
tests/test_cases/
‚îú‚îÄ‚îÄ case_01_service_agreement/       ‚úÖ Clear test case
‚îÇ   ‚îú‚îÄ‚îÄ input/case_01_input_service_agreement.docx
‚îÇ   ‚îú‚îÄ‚îÄ fallback/case_01_fallback_requirements.docx
‚îÇ   ‚îú‚îÄ‚îÄ metadata.json                ‚úÖ Documents expected changes
‚îÇ   ‚îî‚îÄ‚îÄ prompts/ (empty)
‚îú‚îÄ‚îÄ case_02_contract_editing/        ‚úÖ Clear test case
‚îÇ   ‚îú‚îÄ‚îÄ input/case_02_input_service_agreement.docx
‚îÇ   ‚îú‚îÄ‚îÄ fallback/case_02_fallback_comprehensive_guidelines.docx
‚îÇ   ‚îú‚îÄ‚îÄ metadata.json
‚îÇ   ‚îî‚îÄ‚îÄ prompts/ (empty)
‚îî‚îÄ‚îÄ case_03_existing_changes/        ‚úÖ Clear test case
    ‚îú‚îÄ‚îÄ input/case_03_input_with_existing_changes.docx
    ‚îú‚îÄ‚îÄ prompts/case_03_prompt_additional_edits.txt
    ‚îú‚îÄ‚îÄ metadata.json
    ‚îî‚îÄ‚îÄ fallback/ (empty)
```

## Your 3 Test Cases

### üéØ Test Case 01: Service Agreement Tightening
**What:** Upload vague service agreement + requirements document ‚Üí AI should make ~10 specific changes

**Files:**
- Input: `case_01_input_service_agreement.docx` (vague contract)
- Fallback: `case_01_fallback_requirements.docx` (10 requirements with "must/shall")
- Expected changes: 10 (documented in metadata.json)

**Currently:** System doesn't make all 10 changes - **this is what you want to fix!**

---

### üéØ Test Case 02: Complex Contract Editing
**What:** Same input + complex requirements (some don't match input text) ‚Üí AI should make ~9 changes

**Files:**
- Input: `case_02_input_service_agreement.docx` (same as case 01)
- Fallback: `case_02_fallback_comprehensive_guidelines.docx` (insurance, IP, etc.)
- Expected changes: 9+ (some need insertions, not replacements)

**Currently:** Harder test - some requirements have no matching text to replace

---

### üéØ Test Case 03: Existing Tracked Changes ‚ö†Ô∏è CRITICAL
**What:** Doc with 8 existing tracked changes + prompt for 4 new changes ‚Üí Should have 12 total

**Files:**
- Input: `case_03_input_with_existing_changes.docx` (**8 existing tracked changes!**)
- Prompt: `prompts/case_03_prompt_additional_edits.txt` (4 text instructions)
- Expected: 12 total changes (8 original preserved + 4 new)

**Critical:** Original 8 changes MUST NOT be lost!

---

## How to Test (Step by Step)

### 1. Start the application

```bash
# Terminal 1 (from project root)
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 (from project root)
cd frontend-new && npm run dev
```

Open: http://localhost:5173

### 2. Run Test Case 01

1. Click "Upload Document" ‚Üí select `tests/test_cases/case_01_service_agreement/input/case_01_input_service_agreement.docx`
2. Click "Upload Fallback Document" ‚Üí select `tests/test_cases/case_01_service_agreement/fallback/case_01_fallback_requirements.docx`
3. Enable "Debug Mode" and "Extended Debug"
4. Click "Process Document"
5. Download the result
6. Open it and count how many tracked changes were actually made
7. Compare against the 10 expected changes in `metadata.json`

**Document results:**
```json
// In case_01_service_agreement/metadata.json, update:
"baseline_results": {
  "date": "2025-10-27",
  "changes_applied": 7,        // How many you counted
  "changes_missed": 3,          // 10 - 7 = 3 missed
  "missed_changes": [
    "Payment terms (change 3)",
    "Dispute resolution (change 10)"
  ],
  "success_rate": 0.70          // 7/10 = 0.70
}
```

### 3. Run Test Case 03 (Critical!)

1. Upload `case_03_input_with_existing_changes.docx`
2. **Do NOT upload fallback** - instead, copy/paste text from `prompts/case_03_prompt_additional_edits.txt` into the instructions box
3. Click "Process Document"
4. Download result
5. Open and verify:
   - ‚úÖ All 8 original tracked changes still there?
   - ‚úÖ How many of the 4 new changes were added?
6. **If any original changes lost = CRITICAL BUG**

---

## What You'll Learn from Testing

After running baseline tests, you'll have data like:

```
Case 01: 7/10 changes applied (70%) ‚ö†Ô∏è
  Missed: changes 3, 8, 10

Case 02: 5/9 changes applied (56%) ‚ö†Ô∏è
  Missed: changes with no direct text match

Case 03: 8 original + 3 new = 11/12 total (92%) ‚ö†Ô∏è
  Missed: change 4 (counting sequence)
  Critical: ‚úÖ All original changes preserved
```

This tells you:
- Which types of changes the AI struggles with
- Patterns in what gets missed (e.g., multi-word phrases, insertions)
- Where to focus improvement efforts

---

## Next: Iterative Improvement

### The Improvement Loop:

```
1. Test ‚Üí Document what's missed
   ‚Üì
2. Analyze ‚Üí Find patterns
   ‚Üì
3. Fix ‚Üí Update word_processor.py or prompts
   ‚Üì
4. Re-test ‚Üí Measure improvement
   ‚Üì
5. Repeat until success rate > 90%
```

### Common Issues and Fixes:

**Issue:** AI misses changes with phrases that span words
**Fix:** Improve fuzzy matching in `word_processor.py`

**Issue:** AI can't find text that's slightly different in doc
**Fix:** Improve contextual matching using surrounding text

**Issue:** AI doesn't know where to insert new clauses
**Fix:** Improve LLM prompt to better identify insertion points

---

## Files You Should Read

1. **`tests/test_cases/README.md`** - Complete testing guide
2. **`tests/test_cases/INVENTORY.md`** - All files documented
3. **`tests/TEST_CASES.md`** - Comprehensive test documentation
4. **`case_XX/metadata.json`** - Expected changes for each test

---

## Summary of Changes

‚úÖ **Created organized structure** - 3 clear test cases
‚úÖ **Documented expected changes** - 10, 9, and 12 changes respectively
‚úÖ **Preserved original files** - Old locations still work
‚úÖ **Added metadata** - JSON files with all test details
‚úÖ **Created documentation** - 4 markdown files explaining everything

‚ö†Ô∏è **Your next step:** Run baseline tests and document results!

---

## Quick Commands

```bash
# View test case structure
tree tests/test_cases -L 2

# Find all test files
find tests/test_cases -name "*.docx" -o -name "*.txt"

# View metadata for case 01
cat tests/test_cases/case_01_service_agreement/metadata.json | jq

# Analyze case 03 existing changes via API
curl -X POST http://localhost:8000/analyze-document/ \
  -F "file=@tests/test_cases/case_03_existing_changes/input/case_03_input_with_existing_changes.docx"
```

---

## Questions?

See full documentation in:
- `tests/test_cases/README.md`
- `tests/TEST_CASES.md`

**You're ready to start systematic testing!** üöÄ
